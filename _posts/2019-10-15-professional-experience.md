---
layout: post
title: Professional Experience
---

<b>Data Engineer (Data Platform Team) @ Sanford Health</b>
<span class="post-date">July 2024 - Present</span>

* Automated ingestion pipelines using AWS Glue & PySpark, streamlining ETL workflows & reducing manual effort by 25+ hrs/week
* Architected a scalable data lakehouse by redesigning tables & building ingestion pipelines to integrate ERP & Redshift data
* Performed data reconciliation between old & new lakehouses using Spark, ensuring less than 0.01% data loss during migration
* Resolved ServiceNow tickets for custom Power BI reports, modifying existing logic, & fulfilling data pull requests for customers

<b> Student Research Assistant @ University of Massachusetts, Amherst</b>
<span class="post-date">May 2023 - Aug 2023</span>

* Led development of a Python-based data monitoring pipeline using Airflow to track files received from UMass Agriculture School
* Streamlined data hygiene & optimized storage by automating the archival of older files in the HANA transport folder using Airflow


<b>Data Engineer @ AK Fiserv</b>
<span class="post-date">Mar 2022 - Aug 2022</span>

* Built a PySpark & Great Expectations framework to validate 1M+ records daily, ensuring 99.9% accuracy for compliance reporting
* Built CI/CD workflows with GitHub Actions for secure data pipeline deployment & reducing configuration errors by 30%
* Configured IAM roles, S3 policies, & AWS KMS to enforce strict access controls & encryption for financial data, reducing breaches
* Created dashboards to track compliance metrics & audit trails, enabling stakeholders to identify potential risks & take action


<b>Data Engineer (GET) @ Vodafone</b>
<span class="post-date">June 2021 - Feb 2022</span>

* Built a real-time Kafka & Spark Streaming pipeline processing 10M+ events/hr, cutting downtime by 20% via proactive detection
* Designed an ETL pipeline with Airflow & AWS Glue, slashing data prep time from 4 hours to 30 minutes, for TBs of customer data
* Deployed a churn prediction model using PySpark MLlib & SageMaker, achieving 85% accuracy & reducing churn rate by 10%
* Migrated on-premise data to a data lake on S3, improving retrieval times by 30% & enabling scalable storage for 100+ TB of data
* Created dashboards to visualize customer usage patterns & network performance, with optimized dbt models & Airflow DAGs

<b>Data Science Intern @ Iha Consulting</b>
<span class="post-date">Feb 2021 - May 2021</span>

* Engineered a deep learning model for brain tumor segmentation using ResNet-40, VGG-16, etc., achieving 98.3% accuracy

<b>Machine Learning Intern @ Yibe</b>
<span class="post-date">Sep 2020 - Mar 2021</span>

* Developed an image captioning model using encoder-decoder architecture and transfer learning in TensorFlow, achieving a BLEU-1 score of 0.66 on the Flickr30K dataset by integrating InceptionV3 for precise image feature encoding


