<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-03-08T01:03:23+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Anushka Sirpurkar</title><subtitle>Data Engineer @ Sanford Health&lt;br&gt; University of Massachusetts grad</subtitle><entry><title type="html">Hey! I’m Anushka, nice to meet you!</title><link href="http://localhost:4000/2019/10/23/hey-im-anushka.html" rel="alternate" type="text/html" title="Hey! I’m Anushka, nice to meet you!" /><published>2019-10-23T00:00:00+00:00</published><updated>2019-10-23T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/23/hey-im-anushka</id><content type="html" xml:base="http://localhost:4000/2019/10/23/hey-im-anushka.html"><![CDATA[<p>I’m a Data Engineer at Sanford Health with a strong background in business analytics. I work on designing and implementing automated ETL solutions. My passion lies in tackling complex challenges that require deep research and analytical thinking. My professional journey includes valuable experience as a Data Engineer at Vodafone Telecommunications in India, along with successful completion of three major projects and various smaller initiatives during my academic career. I possess strong technical proficiency in modern data technologies including Snowflake, Spark, Python, and AWS services (S3, Redshift, Glue, EC2).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m a Data Engineer at Sanford Health with a strong background in business analytics. I work on designing and implementing automated ETL solutions. My passion lies in tackling complex challenges that require deep research and analytical thinking. My professional journey includes valuable experience as a Data Engineer at Vodafone Telecommunications in India, along with successful completion of three major projects and various smaller initiatives during my academic career. I possess strong technical proficiency in modern data technologies including Snowflake, Spark, Python, and AWS services (S3, Redshift, Glue, EC2).]]></summary></entry><entry><title type="html">Personal Projects</title><link href="http://localhost:4000/2019/10/21/personal-projects.html" rel="alternate" type="text/html" title="Personal Projects" /><published>2019-10-21T00:00:00+00:00</published><updated>2019-10-21T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/21/personal-projects</id><content type="html" xml:base="http://localhost:4000/2019/10/21/personal-projects.html"><![CDATA[<p><b>Predicting Chess Games</b> <a href="https://github.com/aspk74/Chess_EloPos_Evaluation">
[Code]
</a>
<span class="post-date">Dec 2023</span></p>

<ul>
  <li>Designed an automated ETL pipeline using Airflow to pull 3M rows of chess game data from Chess.com APIs, processing this data on GCP buckets</li>
  <li>Transferred stored data from buckets to MongoDB clusters and connected it to a Databricks cluster to develop classification models, leveraging
SparkMLlib to classify game outcomes and surpassing traditional ELO benchmark accuracy by 2.11%</li>
  <li>Used: Airflow, Spark, GCP, Databricks, MongoDB</li>
</ul>

<p><b>GenAI PDF Summarization and Interaction with ChatGPT</b> <a href="https://github.com/aspk74?tab=repositories">
[Code]
</a>
<span class="post-date">May 2024</span></p>

<ul>
  <li>Built GenAI PDF-GPT, a web application using OpenAI’s ChatGPT API and Retrieval-Augmented Generation (RAG) for advanced document analysis</li>
  <li>Extracted and summarized critical insights from uploaded PDF files by using ChatGPT’s robust summarization capabilities for quicker data access
Implemented a chat interface for user interaction with extracted PDF information and used LangChain for efficient information retrieval from PDFs</li>
  <li>Used: Python, OpenAI API, LangChain, RAG, Flask</li>
</ul>

<p><b> Multimodal Resume Analysis</b> <a href="https://github.com/aspk74?tab=repositories">
[Code]
</a>
<span class="post-date">Aug 2023</span></p>

<ul>
  <li>Developed an advanced resume analyzer capable of extracting and analyzing text, images, and audio data from resumes and multimedia presentations</li>
  <li>Engineered a TensorFlow-based pipeline to preprocess multimodal data for real-time classification, achieving processing of upto 500 resumes/minute</li>
  <li>Used: Python, TensorFlow, PyTorch</li>
</ul>

<p><b>Transliteration of Sanskrit Using Natural Language Processing</b> <a href="https://github.com/aspk74?tab=repositories">
[Code]
</a>
<span class="post-date">Jan 2023</span></p>

<ul>
  <li>Designed a platform that understood Sanskrit &amp; performs NLP-based transliteration from English to Sanskrit &amp; performs semantic relation analysis</li>
  <li>Fabricated a complex WordNet using Graph Database for the storage of 65,000 Sanskrit words &amp; built a transliteration module for converting</li>
  <li>Used: Python, NLP, Neo4j</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Predicting Chess Games [Code] Dec 2023 Designed an automated ETL pipeline using Airflow to pull 3M rows of chess game data from Chess.com APIs, processing this data on GCP buckets Transferred stored data from buckets to MongoDB clusters and connected it to a Databricks cluster to develop classification models, leveraging SparkMLlib to classify game outcomes and surpassing traditional ELO benchmark accuracy by 2.11% Used: Airflow, Spark, GCP, Databricks, MongoDB GenAI PDF Summarization and Interaction with ChatGPT [Code] May 2024 Built GenAI PDF-GPT, a web application using OpenAI’s ChatGPT API and Retrieval-Augmented Generation (RAG) for advanced document analysis Extracted and summarized critical insights from uploaded PDF files by using ChatGPT’s robust summarization capabilities for quicker data access Implemented a chat interface for user interaction with extracted PDF information and used LangChain for efficient information retrieval from PDFs Used: Python, OpenAI API, LangChain, RAG, Flask Multimodal Resume Analysis [Code] Aug 2023 Developed an advanced resume analyzer capable of extracting and analyzing text, images, and audio data from resumes and multimedia presentations Engineered a TensorFlow-based pipeline to preprocess multimodal data for real-time classification, achieving processing of upto 500 resumes/minute Used: Python, TensorFlow, PyTorch Transliteration of Sanskrit Using Natural Language Processing [Code] Jan 2023 Designed a platform that understood Sanskrit &amp; performs NLP-based transliteration from English to Sanskrit &amp; performs semantic relation analysis Fabricated a complex WordNet using Graph Database for the storage of 65,000 Sanskrit words &amp; built a transliteration module for converting Used: Python, NLP, Neo4j]]></summary></entry><entry><title type="html">Blogs</title><link href="http://localhost:4000/2019/10/20/blogs.html" rel="alternate" type="text/html" title="Blogs" /><published>2019-10-20T00:00:00+00:00</published><updated>2019-10-20T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/20/blogs</id><content type="html" xml:base="http://localhost:4000/2019/10/20/blogs.html"><![CDATA[<ul>
  <li><a href="https://medium.com/@anushka7400.as/behind-the-scenes-at-lyft-how-airflow-and-flyte-power-data-workflows-7809e9d47ff0">Behind the Scenes at Lyft: How Airflow and Flyte Power Data Workflows</a></li>
  <li>Follow me for for on <a href="https://medium.com/@anushka7400.as">Medium</a></li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Behind the Scenes at Lyft: How Airflow and Flyte Power Data Workflows Follow me for for on Medium]]></summary></entry><entry><title type="html">Publications</title><link href="http://localhost:4000/2019/10/19/publications.html" rel="alternate" type="text/html" title="Publications" /><published>2019-10-19T00:00:00+00:00</published><updated>2019-10-19T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/19/publications</id><content type="html" xml:base="http://localhost:4000/2019/10/19/publications.html"><![CDATA[<ul>
  <li>Applications of IOT in E-toilet Management System, International Journal of Emerging Technology and Research, ISSN:2349-5162 <a href="https://www.jetir.org/view?paper=JETIR2109040">
[Journal link]
</a></li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Applications of IOT in E-toilet Management System, International Journal of Emerging Technology and Research, ISSN:2349-5162 [Journal link]]]></summary></entry><entry><title type="html">Professional Experience</title><link href="http://localhost:4000/2019/10/15/professional-experience.html" rel="alternate" type="text/html" title="Professional Experience" /><published>2019-10-15T00:00:00+00:00</published><updated>2019-10-15T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/15/professional-experience</id><content type="html" xml:base="http://localhost:4000/2019/10/15/professional-experience.html"><![CDATA[<p><b>Data Engineer (Data Platform Team) @ Sanford Health</b>
<span class="post-date">July 2024 - Present</span></p>

<ul>
  <li>Automated CI/CD workflows for ETL processes using Python &amp; AWS Glue for data integration across sources, reducing deployment time by 40%</li>
  <li>Designed ETL pipelines for migrating data from multiple ERPs (Tecsys, Lawson, etc.) to Redshift data warehouse, ensuring system-wide consistency</li>
  <li>Designed multiple real-time data linkage processes for integrating data sources and enabling accurate and low-latency reporting for supply chain metrics</li>
</ul>

<p><b> Student Research Assistant @ University of Massachusetts, Amherst</b>
<span class="post-date">May 2023 - Aug 2023</span></p>

<ul>
  <li>Led development &amp; maintenance of a Python-based data monitoring pipeline using Airflow to track files received from UMass School of Agriculture</li>
  <li>Automated archival of older files in the HANA transport folder using Apache Airflow, ensuring streamlined data hygiene &amp; optimized storage</li>
</ul>

<p><b>Data Engineer @ AK Fiserv</b>
<span class="post-date">Mar 2022 - Aug 2022</span></p>

<ul>
  <li>Built data pipelines for ingesting data from multiple sources(Workday, Peoplesoft, Salesforce, third-party vendor APIs) and load it into Snowflake using
Spark, Python, AWS(S3, EC2, Lambda, Glue), and Snowpipe, etc</li>
  <li>Led cost-saving initiatives by implementing query tags in Airflow DAGs via Databricks &amp; optimizing Snowflake queries, reducing monthly costs by 30%</li>
  <li>Migrated processes from username-password to key-pair authentication for all Spark &amp; Snowflake connectors that reduced vulnerabilities by 80%</li>
  <li>Collaborated with cross-functional teams to discuss Data Profiling requirements and create Source-To-Target-Mapping (STTM) documents</li>
</ul>

<p><b>Data Engineer (GET) @ Vodafone</b>
<span class="post-date">June 2021 - Feb 2022</span></p>

<ul>
  <li>Engineered scalable data pipelines for 30M+ real-time Kafka messages, transforming &amp; loading into Snowflake tables and AWS storage with Prefect,
cutting runtime by 65%</li>
  <li>Implemented a proactive alerting system to swiftly identify server downtime and monitor crucial server metrics, reducing downtime by 80%</li>
  <li>Engineered data transformation workflows to extract 15+ key performance indicators (KPIs), finding critical insights for customer churn cycle</li>
</ul>

<p><b>Machine Learning Intern @ Yibe</b>
<span class="post-date">Sep 2020 - Mar 2021</span></p>

<ul>
  <li>Developed an image captioning model using encoder-decoder architecture and transfer learning in TensorFlow, achieving a BLEU-1 score of 0.66 on
the Flickr30K dataset by integrating InceptionV3 for precise image feature encoding</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Data Engineer (Data Platform Team) @ Sanford Health July 2024 - Present]]></summary></entry><entry><title type="html">More about me</title><link href="http://localhost:4000/2019/10/14/more-about-me.html" rel="alternate" type="text/html" title="More about me" /><published>2019-10-14T00:00:00+00:00</published><updated>2019-10-14T00:00:00+00:00</updated><id>http://localhost:4000/2019/10/14/more-about-me</id><content type="html" xml:base="http://localhost:4000/2019/10/14/more-about-me.html"><![CDATA[<p>I enjoy playing basketball and exploring new desert places during my free time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I enjoy playing basketball and exploring new desert places during my free time.]]></summary></entry></feed>